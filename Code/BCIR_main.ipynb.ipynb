{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import csv\n","import sklearn\n","from sklearn.utils import shuffle\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score\n","import os \n","import cv2\n","import tensorflow as tf\n","import keras\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.layers import Input, MaxPool2D, BatchNormalization, Conv2D, Conv2DTranspose, Dense, Reshape, Flatten, Dropout\n","from tensorflow.keras.models import Model\n","from IPython.display import FileLink\n","from operator import itemgetter\n","import random\n","import math\n","import time\n","\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# insert data path \n","X_40 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\X_classification_40X.npy')\n","y_mainclass_40 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_mainclass_classification_40X.npy')\n","y_subclass_40 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_subclass_classification_40X.npy')\n","\n","X_100 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\X_classification_100X.npy')\n","y_mainclass_100 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_mainclass_classification_100X.npy')\n","y_subclass_100 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_subclass_classification_100X.npy')\n","\n","X_200 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\X_classification_200X.npy')\n","y_mainclass_200 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_mainclass_classification_200X.npy')\n","y_subclass_200 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_subclass_classification_200X.npy')\n","\n","X_400 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\X_classification_400X.npy')\n","y_mainclass_400 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_mainclass_classification_400X.npy')\n","y_subclass_400 = np.load(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\y_subclass_classification_400X.npy')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# shuffling the data\n","X_40, y_mainclass_40, y_subclass_40 = shuffle(X_40, y_mainclass_40, y_subclass_40, random_state=42)\n","\n","X_100, y_mainclass_100, y_subclass_100 = shuffle(X_100, y_mainclass_100, y_subclass_100, random_state=42)\n","\n","X_200, y_mainclass_200, y_subclass_200 = shuffle(X_200, y_mainclass_200, y_subclass_200, random_state=42)\n","\n","X_400, y_mainclass_400, y_subclass_400 = shuffle(X_400, y_mainclass_400, y_subclass_400, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 80% - train, 10% - val, 10% - test\n","X_train_40 = X_40[0 : int(0.8*len(X_40))]\n","y_train_mainclass_40 = y_mainclass_40[0 : int(0.8*len(y_mainclass_40))]\n","y_train_subclass_40 = y_subclass_40[0 : int(0.8*len(y_subclass_40))]\n","\n","X_val_40 = X_40[int(0.8*len(X_40)) : int(0.9*len(X_40))]\n","y_val_mainclass_40 = y_mainclass_40[int(0.8*len(y_mainclass_40)) : int(0.9*len(y_mainclass_40))]\n","y_val_subclass_40 = y_subclass_40[int(0.8*len(y_subclass_40)) : int(0.9*len(y_subclass_40))]\n","\n","X_test_40 = X_40[int(0.9*len(X_40)) : ]\n","y_test_mainclass_40 = y_mainclass_40[int(0.9*len(y_mainclass_40)) : ]\n","y_test_subclass_40 = y_subclass_40[int(0.9*len(y_subclass_40)) : ]\n","\n","################################\n","\n","X_train_100 = X_100[0 : int(0.8*len(X_100))]\n","y_train_mainclass_100 = y_mainclass_100[0 : int(0.8*len(y_mainclass_100))]\n","y_train_subclass_100 = y_subclass_100[0 : int(0.8*len(y_subclass_100))]\n","\n","X_val_100 = X_100[int(0.8*len(X_100)) : int(0.9*len(X_100))]\n","y_val_mainclass_100 = y_mainclass_100[int(0.8*len(y_mainclass_100)) : int(0.9*len(y_mainclass_100))]\n","y_val_subclass_100 = y_subclass_100[int(0.8*len(y_subclass_100)) : int(0.9*len(y_subclass_100))]\n","\n","X_test_100 = X_100[int(0.9*len(X_100)) : ]\n","y_test_mainclass_100 = y_mainclass_100[int(0.9*len(y_mainclass_100)) : ]\n","y_test_subclass_100 = y_subclass_100[int(0.9*len(y_subclass_100)) : ]\n","\n","################################\n","\n","X_train_200 = X_200[0 : int(0.8*len(X_200))]\n","y_train_mainclass_200 = y_mainclass_200[0 : int(0.8*len(y_mainclass_200))]\n","y_train_subclass_200 = y_subclass_200[0 : int(0.8*len(y_subclass_200))]\n","\n","X_val_200 = X_200[int(0.8*len(X_200)) : int(0.9*len(X_200))]\n","y_val_mainclass_200 = y_mainclass_200[int(0.8*len(y_mainclass_200)) : int(0.9*len(y_mainclass_200))]\n","y_val_subclass_200 = y_subclass_200[int(0.8*len(y_subclass_200)) : int(0.9*len(y_subclass_200))]\n","\n","X_test_200 = X_200[int(0.9*len(X_200)) : ]\n","y_test_mainclass_200 = y_mainclass_200[int(0.9*len(y_mainclass_200)) : ]\n","y_test_subclass_200 = y_subclass_200[int(0.9*len(y_subclass_200)) : ]\n","\n","################################\n","\n","X_train_400 = X_400[0 : int(0.8*len(X_400))]\n","y_train_mainclass_400 = y_mainclass_400[0 : int(0.8*len(y_mainclass_400))]\n","y_train_subclass_400 = y_subclass_400[0 : int(0.8*len(y_subclass_400))]\n","\n","X_val_400 = X_400[int(0.8*len(X_400)) : int(0.9*len(X_400))]\n","y_val_mainclass_400 = y_mainclass_400[int(0.8*len(y_mainclass_400)) : int(0.9*len(y_mainclass_400))]\n","y_val_subclass_400 = y_subclass_400[int(0.8*len(y_subclass_400)) : int(0.9*len(y_subclass_400))]\n","\n","X_test_400 = X_400[int(0.9*len(X_400)) : ]\n","y_test_mainclass_400 = y_mainclass_400[int(0.9*len(y_mainclass_400)) : ]\n","y_test_subclass_400 = y_subclass_400[int(0.9*len(y_subclass_400)) : ]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lrelu = tf.keras.layers.LeakyReLU(alpha=0.2)\n","\n","def autoencoder(inputLayer):\n","    # encoder\n","    encoder = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', activation=lrelu)(inputLayer[0])\n","    encoder = BatchNormalization(axis=-1)(encoder)\n","    encoder = MaxPool2D(pool_size=(2,2), padding='same')(encoder)\n","    encoder = Flatten()(encoder)\n","    \n","    latent_space = Dense(48, name='latent_space')(encoder)\n","    \n","    # decoder\n","    decoder = Dense(64*64*128)(latent_space)\n","    decoder = Reshape((64, 64, 128))(decoder)\n","    decoder = Conv2DTranspose(128, kernel_size=(3, 3), strides=(2, 2), padding='same', activation=lrelu)\\\n","    (decoder)\n","    decoder = BatchNormalization(axis=-1)(decoder)\n","    decoder = Conv2DTranspose(3, kernel_size=(3, 3), strides=(2, 2), padding='same', activation=lrelu)\\\n","    (decoder)\n","    \n","    return decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# input = Input(shape=(256, 256, 3), name=\"input\")\n","\n","# z = autoencoder([input])\n","\n","# AUTOENCODER_400 = Model(inputs=[input], outputs=[z])\n","\n","# AUTOENCODER_400.build((None, 256, 256, 3))\n","\n","# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3, decay=2e-5)\n","# AUTOENCODER_400.compile(loss = tf.keras.losses.MeanSquaredError(), optimizer = optimizer, metrics=['accuracy'])\n","\n","\n","# AUTOENCODER_400.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# batch_size = 32\n","# nb_epoch = 100\n","# steps_per_epoch = np.ceil(len(X_train_400)/batch_size)\n","# validation_steps = np.ceil(len(X_val_400)/batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_data(X_train,Y_train,idx,batch_size):\n","    st = idx*batch_size\n","    nrows = st+batch_size\n","    x = X_train[st:nrows]\n","    y = Y_train[st:nrows]\n","    return x,y"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def batch_generator(X_train,Y_train,batch_size,steps):\n","    idx=1\n","    while True:\n","        yield load_data(X_train,Y_train,idx-1,batch_size)\n","        if idx<steps:\n","            idx += 1\n","        else:\n","            idx = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train_batch = batch_generator(X_train_400, X_train_400, batch_size, steps_per_epoch)\n","# valid_batch = batch_generator(X_val_400, X_val_400, batch_size, validation_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# early_40 = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n","# early_100 = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n","# early_200 = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n","# early_400 = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# history_40 = AUTOENCODER_40.fit(train_batch, epochs=nb_epoch, steps_per_epoch=steps_per_epoch, verbose=1,\\\n","#                                 validation_data=valid_batch, validation_steps=validation_steps,callbacks=\\\n","#                                [early_40])\n","\n","# history_100=AUTOENCODER_100.fit(train_batch, epochs=nb_epoch, steps_per_epoch=steps_per_epoch, verbose=1,\\\n","#                                 validation_data=valid_batch, validation_steps=validation_steps,callbacks=\\\n","#                                [early_100])\n","\n","# history_200=AUTOENCODER_200.fit(train_batch, epochs=nb_epoch, steps_per_epoch=steps_per_epoch, verbose=1,\\\n","#                                 validation_data=valid_batch, validation_steps=validation_steps,callbacks=\\\n","#                                [early_200])\n","\n","# history_400=AUTOENCODER_400.fit(train_batch, epochs=nb_epoch, steps_per_epoch=steps_per_epoch, verbose=1,\\\n","#                                 validation_data=valid_batch, validation_steps=validation_steps,callbacks=\\\n","#                                [early_400])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # For visualizing different loss\n","# plt.plot(history_400.history['accuracy'])\n","# plt.plot(history_400.history['val_accuracy'])\n","# plt.title(\"model_accuracy_400X\")\n","# plt.ylabel('Accuracy')\n","# plt.xlabel('Epoch')\n","# plt.legend(['accuracy','val_accuracy'])\n","# plt.show()\n","\n","# plt.plot(history_400.history['loss'])\n","# plt.plot(history_400.history['val_loss'])\n","# plt.title(\"model_loss_400X\")\n","# plt.ylabel('Loss')\n","# plt.xlabel('Epoch')\n","# plt.legend(['loss','val_loss'])\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# AUTOENCODER_400.save('Model_400X.h5')\n","\n","# FileLink('Model_400X.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["AUTOENCODER_40 = tf.keras.models.load_model(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\MODELS\\Model_40X.h5')\n","AUTOENCODER_100= tf.keras.models.load_model(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\MODELS\\Model_100X.h5')\n","AUTOENCODER_200= tf.keras.models.load_model(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\MODELS\\Model_200X.h5')\n","AUTOENCODER_400= tf.keras.models.load_model(r'D:\\INTERNSHIPS_PROJECTS\\PROJECTS\\Breast Cancer Image Retrieval\\MODELS\\Model_400X.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# obtaining encoder from trained autoencoder\n","AUTOENCODER_40.trainable = False\n","AUTOENCODER_100.trainable = False\n","AUTOENCODER_200.trainable = False\n","AUTOENCODER_400.trainable = False\n","\n","encoder_40 = Model(inputs=AUTOENCODER_40.input, outputs=AUTOENCODER_40.layers[5].output)\n","encoder_100 = Model(inputs=AUTOENCODER_100.input, outputs=AUTOENCODER_100.layers[5].output)\n","encoder_200 = Model(inputs=AUTOENCODER_200.input, outputs=AUTOENCODER_200.layers[5].output)\n","encoder_400 = Model(inputs=AUTOENCODER_400.input, outputs=AUTOENCODER_400.layers[5].output)\n","\n","print(encoder_40(X_train_40[9].reshape(-1, 256, 256, 3)))\n","print(encoder_100(X_train_100[9].reshape(-1, 256, 256, 3)))\n","print(encoder_200(X_train_200[9].reshape(-1, 256, 256, 3)))\n","print(encoder_400(X_train_400[9].reshape(-1, 256, 256, 3)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(X_test_40[121])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(AUTOENCODER_40(X_test_40[121].reshape(-1, 256, 256, 3))[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(AUTOENCODER_100(X_test_40[121].reshape(-1, 256, 256, 3))[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(AUTOENCODER_200(X_test_40[121].reshape(-1, 256, 256, 3))[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(AUTOENCODER_400(X_test_40[121].reshape(-1, 256, 256, 3))[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# feature extraction of all training images \n","\n","# for mainclass and subclass\n","train_features_40 = []\n","for index, image in enumerate(X_train_40):\n","    dict_main = {}\n","    dict_main[\"index\"] = str(index)\n","    dict_main[\"main_label\"] = str(y_train_mainclass_40[index])\n","    dict_main[\"sub_label\"] = str(y_train_subclass_40[index])\n","    encoding_40 = encoder_40(X_train_40[index].reshape(-1, 256, 256, 3))\n","    dict_main[\"features_40\"] = encoding_40\n","    train_features_40.append(dict_main)\n","\n","train_features_100 = []\n","for index, image in enumerate(X_train_100):\n","    dict_main = {}\n","    dict_main[\"index\"] = str(index)\n","    dict_main[\"main_label\"] = str(y_train_mainclass_100[index])\n","    dict_main[\"sub_label\"] = str(y_train_subclass_100[index])\n","    encoding_100 = encoder_100(X_train_100[index].reshape(-1, 256, 256, 3))\n","    dict_main[\"features_100\"] = encoding_100\n","    train_features_100.append(dict_main)\n","\n","train_features_200 = []\n","for index, image in enumerate(X_train_200):\n","    dict_main = {}\n","    dict_main[\"index\"] = str(index)\n","    dict_main[\"main_label\"] = str(y_train_mainclass_200[index])\n","    dict_main[\"sub_label\"] = str(y_train_subclass_200[index])\n","    encoding_200 = encoder_200(X_train_200[index].reshape(-1, 256, 256, 3))\n","    dict_main[\"features_200\"] = encoding_200\n","    train_features_200.append(dict_main)\n","\n","train_features_400 = []\n","for index, image in enumerate(X_train_400):\n","    dict_main = {}\n","    dict_main[\"index\"] = str(index)\n","    dict_main[\"main_label\"] = str(y_train_mainclass_400[index])\n","    dict_main[\"sub_label\"] = str(y_train_subclass_400[index])\n","    encoding_400 = encoder_400(X_train_400[index].reshape(-1, 256, 256, 3))\n","    dict_main[\"features_400\"] = encoding_400\n","    train_features_400.append(dict_main)\n","\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Retrieval_from_40X(query_image):\n","    query_features = encoder_40(query_image.reshape(-1, 256, 256, 3))\n","    cosine_similarities = []\n","\n","    for feature in train_features_40:\n","        dict_cosine = {}\n","        CS = tf.keras.metrics.CosineSimilarity(name='cosine_similarity')\n","        CS.update_state(feature[\"features_40\"], query_features)\n","        dict_cosine[\"index\"] = feature[\"index\"]\n","        dict_cosine[\"cosine_simi\"] = CS.result().numpy()\n","        dict_cosine[\"main_label\"] = feature[\"main_label\"]\n","        dict_cosine[\"sub_label\"] = feature[\"sub_label\"]\n","        CS.reset_state()\n","        cosine_similarities.append(dict_cosine)\n","    \n","    cosine_similarities = sorted(cosine_similarities, key=itemgetter(\"cosine_simi\"), reverse=True)\n","    \n","    # for retrieved training images\n","    indices = [] \n","    main_labels = []\n","    sub_labels = []\n","    \n","    for _ in cosine_similarities[:10]:\n","        indices.append(int(_[\"index\"]))\n","        main_labels.append(int(_[\"main_label\"]))\n","        sub_labels.append(int(_[\"sub_label\"]))\n","    \n","    return indices, main_labels, sub_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Retrieval_from_100X(query_image):\n","    query_features = encoder_100(query_image.reshape(-1, 256, 256, 3))\n","    cosine_similarities = []\n","\n","    for feature in train_features_100:\n","        dict_cosine = {}\n","        CS = tf.keras.metrics.CosineSimilarity(name='cosine_similarity')\n","        CS.update_state(feature[\"features_100\"], query_features)\n","        dict_cosine[\"index\"] = feature[\"index\"]\n","        dict_cosine[\"cosine_simi\"] = CS.result().numpy()\n","        dict_cosine[\"main_label\"] = feature[\"main_label\"]\n","        dict_cosine[\"sub_label\"] = feature[\"sub_label\"]\n","        CS.reset_state()\n","        cosine_similarities.append(dict_cosine)\n","    \n","    cosine_similarities = sorted(cosine_similarities, key=itemgetter(\"cosine_simi\"), reverse=True)\n","    \n","    # for retrieved training images\n","    indices = [] \n","    main_labels = []\n","    sub_labels = []\n","    \n","    for _ in cosine_similarities[:10]:\n","        indices.append(int(_[\"index\"]))\n","        main_labels.append(int(_[\"main_label\"]))\n","        sub_labels.append(int(_[\"sub_label\"]))\n","    \n","    return indices, main_labels, sub_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Retrieval_from_200X(query_image):\n","    query_features = encoder_200(query_image.reshape(-1, 256, 256, 3))\n","    cosine_similarities = []\n","\n","    for feature in train_features_200:\n","        dict_cosine = {}\n","        CS = tf.keras.metrics.CosineSimilarity(name='cosine_similarity')\n","        CS.update_state(feature[\"features_200\"], query_features)\n","        dict_cosine[\"index\"] = feature[\"index\"]\n","        dict_cosine[\"cosine_simi\"] = CS.result().numpy()\n","        dict_cosine[\"main_label\"] = feature[\"main_label\"]\n","        dict_cosine[\"sub_label\"] = feature[\"sub_label\"]\n","        CS.reset_state()\n","        cosine_similarities.append(dict_cosine)\n","    \n","    cosine_similarities = sorted(cosine_similarities, key=itemgetter(\"cosine_simi\"), reverse=True)\n","    \n","    # for retrieved training images\n","    indices = [] \n","    main_labels = []\n","    sub_labels = []\n","    \n","    for _ in cosine_similarities[:10]:\n","        indices.append(int(_[\"index\"]))\n","        main_labels.append(int(_[\"main_label\"]))\n","        sub_labels.append(int(_[\"sub_label\"]))\n","    \n","    return indices, main_labels, sub_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Retrieval_from_400X(query_image):\n","    query_features = encoder_400(query_image.reshape(-1, 256, 256, 3))\n","    cosine_similarities = []\n","\n","    for feature in train_features_400:\n","        dict_cosine = {}\n","        CS = tf.keras.metrics.CosineSimilarity(name='cosine_similarity')\n","        CS.update_state(feature[\"features_400\"], query_features)\n","        dict_cosine[\"index\"] = feature[\"index\"]\n","        dict_cosine[\"cosine_simi\"] = CS.result().numpy()\n","        dict_cosine[\"main_label\"] = feature[\"main_label\"]\n","        dict_cosine[\"sub_label\"] = feature[\"sub_label\"]\n","        CS.reset_state()\n","        cosine_similarities.append(dict_cosine)\n","    \n","    cosine_similarities = sorted(cosine_similarities, key=itemgetter(\"cosine_simi\"), reverse=True)\n","    \n","    # for retrieved training images\n","    indices = [] \n","    main_labels = []\n","    sub_labels = []\n","    \n","    for _ in cosine_similarities[:10]:\n","        indices.append(int(_[\"index\"]))\n","        main_labels.append(int(_[\"main_label\"]))\n","        sub_labels.append(int(_[\"sub_label\"]))\n","    \n","    return indices, main_labels, sub_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def metrics_from_40X():\n","    precision_main = []\n","    precision_sub = []\n","    recall_main = []\n","    recall_sub = []\n","    \n","    for index, query in enumerate(X_test_40):\n","        indices, main_labels, sub_labels = Retrieval_from_40X(query)\n","        query_main = [y_test_mainclass_40[index]]*10\n","        query_sub = [y_test_subclass_40[index]]*10\n","        \n","        query_main = tf.one_hot(query_main, 2)\n","        main_labels = tf.one_hot(main_labels, 2)\n","        query_sub = tf.one_hot(query_sub, 8)\n","        sub_labels = tf.one_hot(sub_labels, 8)\n","        \n","        precision_main.append(precision_score(query_main, main_labels, average='micro'))\n","        precision_sub.append(precision_score(query_sub, sub_labels, average='micro'))\n","        recall_main.append(recall_score(query_main, main_labels, average='micro'))\n","        recall_sub.append(recall_score(query_sub, sub_labels, average='micro'))\n","        \n","    avg_precision_main = np.mean(np.array(precision_main))\n","    avg_precision_sub = np.mean(np.array(precision_sub))\n","    avg_recall_main = np.mean(np.array(recall_main))\n","    avg_recall_sub = np.mean(np.array(recall_sub))\n","    \n","    return avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def metrics_from_100X():\n","    precision_main = []\n","    precision_sub = []\n","    recall_main = []\n","    recall_sub = []\n","    \n","    for index, query in enumerate(X_test_100):\n","        indices, main_labels, sub_labels = Retrieval_from_100X(query)\n","        query_main = [y_test_mainclass_100[index]]*10\n","        query_sub = [y_test_subclass_100[index]]*10\n","        \n","        query_main = tf.one_hot(query_main, 2)\n","        main_labels = tf.one_hot(main_labels, 2)\n","        query_sub = tf.one_hot(query_sub, 8)\n","        sub_labels = tf.one_hot(sub_labels, 8)\n","        \n","        precision_main.append(precision_score(query_main, main_labels, average='micro'))\n","        precision_sub.append(precision_score(query_sub, sub_labels, average='micro'))\n","        recall_main.append(recall_score(query_main, main_labels, average='micro'))\n","        recall_sub.append(recall_score(query_sub, sub_labels, average='micro'))\n","        \n","    avg_precision_main = np.mean(np.array(precision_main))\n","    avg_precision_sub = np.mean(np.array(precision_sub))\n","    avg_recall_main = np.mean(np.array(recall_main))\n","    avg_recall_sub = np.mean(np.array(recall_sub))\n","    \n","    return avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def metrics_from_200X():\n","    precision_main = []\n","    precision_sub = []\n","    recall_main = []\n","    recall_sub = []\n","    \n","    for index, query in enumerate(X_test_200):\n","        indices, main_labels, sub_labels = Retrieval_from_200X(query)\n","        query_main = [y_test_mainclass_200[index]]*10\n","        query_sub = [y_test_subclass_200[index]]*10\n","        \n","        query_main = tf.one_hot(query_main, 2)\n","        main_labels = tf.one_hot(main_labels, 2)\n","        query_sub = tf.one_hot(query_sub, 8)\n","        sub_labels = tf.one_hot(sub_labels, 8)\n","        \n","        precision_main.append(precision_score(query_main, main_labels, average='micro'))\n","        precision_sub.append(precision_score(query_sub, sub_labels, average='micro'))\n","        recall_main.append(recall_score(query_main, main_labels, average='micro'))\n","        recall_sub.append(recall_score(query_sub, sub_labels, average='micro'))\n","        \n","    avg_precision_main = np.mean(np.array(precision_main))\n","    avg_precision_sub = np.mean(np.array(precision_sub))\n","    avg_recall_main = np.mean(np.array(recall_main))\n","    avg_recall_sub = np.mean(np.array(recall_sub))\n","    \n","    return avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def metrics_from_400X():\n","    precision_main = []\n","    precision_sub = []\n","    recall_main = []\n","    recall_sub = []\n","    \n","    for index, query in enumerate(X_test_400):\n","        indices, main_labels, sub_labels = Retrieval_from_400X(query)\n","        query_main = [y_test_mainclass_400[index]]*10\n","        query_sub = [y_test_subclass_400[index]]*10\n","        \n","        query_main = tf.one_hot(query_main, 2)\n","        main_labels = tf.one_hot(main_labels, 2)\n","        query_sub = tf.one_hot(query_sub, 8)\n","        sub_labels = tf.one_hot(sub_labels, 8)\n","        \n","        precision_main.append(precision_score(query_main, main_labels, average='micro'))\n","        precision_sub.append(precision_score(query_sub, sub_labels, average='micro'))\n","        recall_main.append(recall_score(query_main, main_labels, average='micro'))\n","        recall_sub.append(recall_score(query_sub, sub_labels, average='micro'))\n","        \n","    avg_precision_main = np.mean(np.array(precision_main))\n","    avg_precision_sub = np.mean(np.array(precision_sub))\n","    avg_recall_main = np.mean(np.array(recall_main))\n","    avg_recall_sub = np.mean(np.array(recall_sub))\n","    \n","    return avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub = metrics_from_40X()\n","\n","# print(\"METRICS ON 40X DATA\")\n","print(\"PRECISION MAIN -->\", avg_precision_main)\n","print(\"PRECISION  SUB -->\", avg_precision_sub)\n","print(\"RECALL MAIN -->\", avg_recall_main)\n","print(\"RECALL  SUB -->\", avg_recall_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub = metrics_from_100X()\n","\n","# print(\"METRICS ON 100X DATA\")\n","print(\"PRECISION MAIN -->\", avg_precision_main)\n","print(\"PRECISION  SUB -->\", avg_precision_sub)\n","print(\"RECALL MAIN -->\", avg_recall_main)\n","print(\"RECALL  SUB -->\", avg_recall_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub = metrics_from_200X()\n","\n","# print(\"METRICS ON 200X DATA\")\n","print(\"PRECISION MAIN -->\", avg_precision_main)\n","print(\"PRECISION  SUB -->\", avg_precision_sub)\n","print(\"RECALL MAIN -->\", avg_recall_main)\n","print(\"RECALL  SUB -->\", avg_recall_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["avg_precision_main, avg_precision_sub, avg_recall_main, avg_recall_sub = metrics_from_400X()\n","\n","# print(\"METRICS ON 400X DATA\")\n","print(\"PRECISION MAIN -->\", avg_precision_main)\n","print(\"PRECISION  SUB -->\", avg_precision_sub)\n","print(\"RECALL MAIN -->\", avg_recall_main)\n","print(\"RECALL  SUB -->\", avg_recall_sub)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MAIN_CLASS_LABELS = [\"benign\", \"malignant\"]\n","SUB_CLASS_LABELS = [\"adenosis\", \"fibroadenoma\", \"phyllodes_tumor\", \"tubular_adenoma\",\\\n","                   \"ductal_carcinoma\", \"lobular_carcinoma\", \"mucinous_carcinoma\", \"papillary_carcinoma\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# query image\n","query_index = 178\n","\n","query_image = X_test_200[query_index]\n","\n","plt.imshow(query_image)\n","plt.title(MAIN_CLASS_LABELS[y_test_mainclass_200[query_index]]+\" (\"+SUB_CLASS_LABELS[y_test_subclass_200[query_index]]+\")\",fontsize=10)\n","# print(y_test_mainclass_40[query_index])\n","# plt.title('Query Image')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(10, 6))\n","\n","indices, _mc, _sc = Retrieval_from_40X(query_image)\n","\n","columns = 5\n","rows = 2\n","for i in range(1, columns*rows +1):\n","    img = X_train_40[indices[i-1]]\n","    fig.add_subplot(rows, columns, i)\n","    plt.title(MAIN_CLASS_LABELS[_mc[i-1]]+\" (\"+SUB_CLASS_LABELS[_sc[i-1]]+\")\", fontsize=7.5)\n","    plt.imshow(img)\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(10, 6))\n","\n","indices, _mc, _sc = Retrieval_from_100X(query_image)\n","\n","columns = 5\n","rows = 2\n","for i in range(1, columns*rows +1):\n","    img = X_train_100[indices[i-1]]\n","    fig.add_subplot(rows, columns, i)\n","    plt.title(MAIN_CLASS_LABELS[_mc[i-1]]+\" (\"+SUB_CLASS_LABELS[_sc[i-1]]+\")\", fontsize=7.5)\n","    plt.imshow(img)\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(10, 6))\n","\n","indices, _mc, _sc = Retrieval_from_200X(query_image)\n","\n","columns = 5\n","rows = 2\n","for i in range(1, columns*rows +1):\n","    img = X_train_200[indices[i-1]]\n","    fig.add_subplot(rows, columns, i)\n","    plt.title(MAIN_CLASS_LABELS[_mc[i-1]]+\" (\"+SUB_CLASS_LABELS[_sc[i-1]]+\")\", fontsize=7.5)\n","    plt.imshow(img)\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize=(10, 6))\n","\n","indices, _mc, _sc = Retrieval_from_400X(query_image)\n","\n","columns = 5\n","rows = 2\n","for i in range(1, columns*rows +1):\n","    img = X_train_400[indices[i-1]]\n","    fig.add_subplot(rows, columns, i)\n","    plt.title(MAIN_CLASS_LABELS[_mc[i-1]]+\" (\"+SUB_CLASS_LABELS[_sc[i-1]]+\")\", fontsize=7.5)\n","    plt.imshow(img)\n","    plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
